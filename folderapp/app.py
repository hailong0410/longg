import os
os.environ["KMP_DUPLICATE_LIB_OK"] = "TRUE"

import streamlit as st
import cv2
import numpy as np
from PIL import Image
from facenet_pytorch import MTCNN
from emotiefflib.facial_analysis import EmotiEffLibRecognizer, get_model_list

def recognize_faces(frame: np.ndarray, device: str):
    mtcnn = MTCNN(keep_all=False, post_process=False, min_face_size=40, device=device)
    boxes, probs = mtcnn.detect(frame, landmarks=False)
    if boxes is None or probs is None:
        return []
    boxes = boxes[probs > 0.9]
    faces = []
    for box in boxes:
        x1, y1, x2, y2 = box.astype(int)
        faces.append(frame[y1:y2, x1:x2, :])
    return faces

class RealTimeEmotionRecognizer:
    def __init__(self, model_name=None, device="cpu"):
        if model_name is None:
            model_name = get_model_list()[0]
        self.device = device
        self.recognizer = EmotiEffLibRecognizer(
            engine="onnx",
            model_name=model_name,
            device=device
        )
        self.mtcnn = MTCNN(keep_all=False, post_process=False, min_face_size=40, device=device)

    def process_frame(self, frame_bgr):
        rgb = cv2.cvtColor(frame_bgr, cv2.COLOR_BGR2RGB)
        faces = recognize_faces(rgb, self.device)
        if len(faces) == 0:
            return frame_bgr, None  # Kh√¥ng ph√°t hi·ªán m·∫∑t

        emotions, _ = self.recognizer.predict_emotions(faces, logits=False)
        # V·∫Ω bounding box v√† label l√™n frame
        for bbox, emo in zip(self.mtcnn.detect(rgb)[0], emotions):
            if bbox is not None:
                x1, y1, x2, y2 = bbox.astype(int)
                cv2.rectangle(frame_bgr, (x1, y1), (x2, y2), (0, 255, 0), 2)
                cv2.putText(frame_bgr, emo, (x1, y1 - 10), cv2.FONT_HERSHEY_SIMPLEX,
                            0.9, (0, 255, 0), 2)
        return frame_bgr, emotions[0]  # Tr·∫£ v·ªÅ frame ƒë√£ v·∫Ω v√† emotion ƒë·∫ßu ti√™n

def main():
    st.title("üé• Nh·∫≠n di·ªán c·∫£m x√∫c t·ª´ camera")

    # Ch·ªçn thi·∫øt b·ªã v√† model
    device = st.sidebar.selectbox("Ch·ªçn thi·∫øt b·ªã", ["cpu", "cuda"])
    model_name = st.sidebar.selectbox("Ch·ªçn model", get_model_list())

    recognizer = RealTimeEmotionRecognizer(model_name=model_name, device=device)

    # D√πng camera_input capture ·∫£nh tƒ©nh
    img_file = st.camera_input("Ch·ª•p ·∫£nh webcam ƒë·ªÉ nh·∫≠n di·ªán c·∫£m x√∫c")
    if img_file is not None:
        # PIL Image ‚Üí np.array ‚Üí BGR
        pil_img = Image.open(img_file)
        rgb_img = np.array(pil_img)
        bgr_img = cv2.cvtColor(rgb_img, cv2.COLOR_RGB2BGR)

        frame_proc, emotion = recognizer.process_frame(bgr_img)
        # Convert BGR ‚Üí RGB ƒë·ªÉ hi·ªÉn th·ªã
        disp = cv2.cvtColor(frame_proc, cv2.COLOR_BGR2RGB)
        if emotion:
            st.image(disp, caption=f"C·∫£m x√∫c ph√°t hi·ªán: {emotion}", use_container_width=True)
        else:
            st.image(disp, caption="Kh√¥ng ph√°t hi·ªán khu√¥n m·∫∑t", use_column_width=True)
if __name__ == "__main__":
    main()
